{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Implementation of Deep  neural netwrok with arbitary hidden layers and units.\n",
    "\n",
    "(a) Test your network with the same training data that you used in Problem 2, using both 1D and higher dimensional data. Experiment with using 3 and 5 hidden layers. Evaluate the accuracy of your solutions in the same way as Problem 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sol:** Here we implement designing a deep neural network which can handle arbitary inputs,dimensions, hidden layer and hidden units.  \n",
    "#### Loss Function\n",
    "Regression loss function as given in the question.\n",
    "\n",
    "#### Input \n",
    "Inputs are generated randomly by setting the number of training inputs and the dimensions.\n",
    "\n",
    "#### Outputs \n",
    "The oputputs are limited to a single output. Here I have used **y=sin(x)** function to produce the ouputs.\n",
    "If the there is a n dimensional input, output is a function of sin() for the first dimension and an summation of all the other subsequent dimensions.\n",
    "\n",
    "#### Hyper Paramters\n",
    "For this function there are four hyper paramters, that is the **Learning Rate** (Step Size), the number of **epochs** , the number of **hidden layers** and the number of **hidden units** in each hidden layer.\n",
    "\n",
    "\n",
    "#### The Netwrok has 6 main Functions\n",
    "1) Network Initialization\n",
    "2) nonlin \n",
    "3) feed_forward\n",
    "4) back_prop\n",
    "5) update_network \n",
    "6) train\n",
    "\n",
    "**Now lets got thorugh each function**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the Network\n",
    "def initialize_network(n_inputs,dimension, hidden_in_each_layer):\n",
    "    if (dimension==1):\n",
    "        x= np.random.randn(n_inputs,1) ## Randomly Genrated Inputs\n",
    "        y= np.sin(x)                   ## Generating Outputs\n",
    "    else:\n",
    "## Breaking the inputs such that, the first input goes to the sin curve and the rest, adds on as linear functions.        \n",
    "        x= np.random.randn(n_inputs,dimension) ## Randomly Genrated Inputs\n",
    "        x1=np.reshape(x[:,0],(n_inputs,1)) \n",
    "        x2 = np.sum(x[:,1:],axis=1,keepdims= True) \n",
    "        y= np.sin(x1) + x2\n",
    "## Initializing Weights and Biases by using dictionary\n",
    "    n_hidden_layers = len(hidden_in_each_layer)\n",
    "    wts={'w1':np.random.randn(dimension,hidden_in_each_layer[0]) }\n",
    "    b = {'b1':np.random.randn(hidden_in_each_layer[0],1)}\n",
    "    for i in range(n_hidden_layers-1):\n",
    "        wts['w'+str(i+2)]= np.random.randn(hidden_in_each_layer[i],hidden_in_each_layer[i+1])      \n",
    "        b['b'+str(i+2)] = np.random.randn(hidden_in_each_layer[i+1],1)\n",
    "    wts['w'+str(n_hidden_layers+1)] = np.random.randn(hidden_in_each_layer[-1],1) \n",
    "    b['b'+str(n_hidden_layers+1)] = np.random.randn(1)\n",
    "    return x,y,wts,b\n",
    "\n",
    "## Non Linear Function RELU\n",
    "# def nonlin(x, deriv = False):\n",
    "#     if (deriv == True):\n",
    "#         return 1*(x>0)\n",
    "#     return np.maximum(0,x)\n",
    "\n",
    "def nonlin(x, deriv = False):\n",
    "    if (deriv == True):\n",
    "        return np.ones(x.shape)\n",
    "    return x\n",
    "\n",
    "## Feed Forward Function\n",
    "def feed_forward(x,wts,b):\n",
    "    a = {'a1':(x.dot(wts['w1']).transpose() + b['b1']).transpose()}\n",
    "    for i in range(len(wts)-2):\n",
    "        a['a'+str(i+2)] = nonlin((a['a'+str(i+1)].dot(wts['w'+str(i+2)]).transpose() + b['b'+str(i+2)]).transpose())\n",
    "    a['al'] = a['a'+str(len(wts)-1)].dot(wts['w'+str(len(wts))])+ b['b'+str(len(wts))] \n",
    "    return a\n",
    "\n",
    "## Back Propogation by appling the four main formulas of back propogation\n",
    "def back_prop(x,y,a,wts):\n",
    "    error = (((y-a['al'])**2).sum())                    ## Calculating the error\n",
    "    d = {'d'+str(len(wts)): a['al'] - y}                ## Delta for final layer\n",
    "    wt_err = {'wr'+str(len(wts)):a['a'+str(len(wts)-1)].T.dot(d['d'+str(len(wts))])} ## delta w for final layer\n",
    "    b_err = {'br'+str(len(wts)):d['d'+str(len(wts))].sum()}                          ## delta b for final layer\n",
    "    for i in reversed(range(1,len(wts))):\n",
    "        d ['d'+str(i)] = (wts['w'+ str(i+1)].dot(d['d'+str(i+1)].transpose())).transpose()*((nonlin(a['a'+str(i)],True)))\n",
    "        if (i==1):\n",
    "            wt_err['wr'+str(i)] = x.T.dot(d['d'+str(i)])\n",
    "            b_err['br'+str(i)] = np.sum(d['d'+str(i)].transpose(),axis=1,keepdims= True) \n",
    "        else:\n",
    "            wt_err['wr'+str(i)] = a['a'+str(i-1)].T.dot(d['d'+str(i)])\n",
    "            b_err['br'+str(i)] = np.sum(d['d'+str(i)].transpose(),axis=1,keepdims= True) \n",
    "#     print(error)\n",
    "    return wt_err, b_err, d, error\n",
    "\n",
    "## Updating the Network\n",
    "def update_network(wts,b,wt_err,b_err,lr):\n",
    "    for i in range(0,len(wts)):\n",
    "        wts['w'+str(i+1)] -= lr * wt_err['wr'+str(i+1)]\n",
    "        b['b'+str(i+1)] -= lr * b_err['br'+str(i+1)]\n",
    "    return wts,b\n",
    "\n",
    "## Training the Network\n",
    "def train(net,epochs,lr):\n",
    "    error = []\n",
    "    (wts,b) = (net[2],net[3])\n",
    "    for i in range(epochs):\n",
    "        a = feed_forward(net[0],wts,b)\n",
    "        err = back_prop(net[0], net[1],a,wts)\n",
    "        error.append(err[3])\n",
    "        update = update_network(wts,b,err[0],err[1],lr)\n",
    "        (wts,b)= (update[0],update[1])\n",
    "    return error, update[0], update[1],a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)Defining the Main Function and Implementing for 1D input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import exp\n",
    "\n",
    "## Defining the inputs\n",
    "## hidden_in_each_layer is such that: the number of entries\n",
    "# define the number of hidden layers and each entered value is the number of hidden units in that layer\n",
    "(n_inputs,dimension,hidden_in_each_layer) =(1,1,[1,1]) ## Use [3,6,5] for good accuracy with 3 layers\n",
    "np.random.seed(1)                                          ## Use [3,6,6,6,2] for good accuracy with 5 layers\n",
    "\n",
    "# Initializing the Network\n",
    "net = initialize_network(n_inputs,dimension, hidden_in_each_layer)\n",
    "\n",
    "## Hyper Parameter Initialization\n",
    "epochs= 10000\n",
    "lr = 0.0001\n",
    "# net[0]\n",
    "# print(net[1])\n",
    "\n",
    "## Train\n",
    "# tr = train (net,epochs,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.36782234]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two Layer \n",
    "(wts,b) = (net[2],net[3])\n",
    "a = feed_forward(net[0],wts,b)\n",
    "a['al']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.00519654]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two Layer \n",
    "(wts,b) = (net[2],net[3])\n",
    "a = feed_forward(net[0],wts,b)\n",
    "a['al']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'al2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-877121b565e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sum of Differences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mal2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'al'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mal3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'al'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'al2' is not defined"
     ]
    }
   ],
   "source": [
    "# Sum of Differences\n",
    "al2['al'] - al3['al']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(init['al']-init1['al']).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(wts,b) = (net[2],net[3])\n",
    "a = feed_forward(net[0],wts,b)\n",
    "a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(wts,b) = (net[2],net[3])\n",
    "a = feed_forward(net[0],wts,b)\n",
    "a\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Graphs \n",
    "<span style=\"color:red\"></style>\n",
    "**Red: Ground Truth Values **\n",
    "\n",
    "<span style=\"color:blue\"></style>\n",
    "**Blue: Predicted Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_test = 500\n",
    "dimension=1\n",
    "## Plotting the Outputs vs Inputs\n",
    "if (dimension==1):\n",
    "        test_input= np.random.randn(n_test,1) ## Randomly Genrated Inputs\n",
    "        gtruth = np.sin(test_input)\n",
    "else:\n",
    "## Breaking the inputs such that, the first input goes to the sin curve and the rest adds on as linear functions        \n",
    "        x= np.random.randn(n_inputs,dimension) ## Randomly Genrated Inputs\n",
    "        x1=np.reshape(x[:,0],(n_inputs,1)) \n",
    "        x2 = np.sum(x[:,1:],axis=1,keepdims= True) \n",
    "        y= np.sin(x1) + x2\n",
    "plt.plot(test_input,gtruth,'r.',markersize=12)\n",
    "\n",
    "# Caluclating Predictions from the network\n",
    "predict= feed_forward(test_input,tr[1],tr[2])['al']\n",
    "\n",
    "plt.plot(test_input,predict ,'b.')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the Error\n",
    "plt.title('Final Error  ' + str(tr[0][-1]))\n",
    "plt.plot(tr[0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Inferences\n",
    "1. Comaparing to the previous questions, the number of epochs increased as the number of hidden layers increased. \n",
    "2. Varying the hidden units has a significant effect on the output. From experiments, the hidden units should be small in the intial layer, increase in the middle layers and again decrease in the final layers.\n",
    "3. As the number of parameters are more, decreasing the learning rate and increasing the epochs helped.\n",
    "\n",
    "#### Comparing 3 and 5 layer hidden networks\n",
    "1. I was able to get a good prediction of the sin() function by tuning the params to lr= 0.001; epochs = 1000 and with 3 hidden layers with 3, 6 and 5 hidden units respectively.\n",
    "2. We can see that the predictions in the initial points and final points is not accurate as, the training data here is also very less, therefore poor predictions.\n",
    "2. For 5 layers: epochs = 10,000; lr= 0.0001; hidden units and layers: [3,6,6,6,2].\n",
    "3. When using 5 hidden layers, increasing the epochs to 10,000 and decreasing the learning rate to 0.0001 helped. This is due to the fact that the number of weights significantly increase.\n",
    "4. keeping the number of hidden units smaller helps getting a better accuracy fot both 3 and 5 layer hidden network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Main Function and Implementing for nD input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Importing the Libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import exp\n",
    "\n",
    "## Defining the inputs\n",
    "## hidden_in_each_layer is such that: the number of entries\n",
    "# define the number of hidden layers and each entered value is the number of hidden units in that layer\n",
    "(n_inputs,dimension,hidden_in_each_layer) =(100,2,[25,15]) \n",
    "np.random.seed(1)\n",
    "\n",
    "# Initializing the Network\n",
    "net = initialize_network(n_inputs,dimension, hidden_in_each_layer)\n",
    "\n",
    "## Hyper Parameter Initialization\n",
    "epochs= 100000\n",
    "lr = 0.0001\n",
    "# net[0]\n",
    "# print(net[1])\n",
    "\n",
    "## Train\n",
    "tr = train (net,epochs,lr)\n",
    "\n",
    "# Plotting the Error\n",
    "plt.title('Final Error  ' + str(tr[0][-1]))\n",
    "plt.plot(tr[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Graphs \n",
    "<span style=\"color:red\"></style>\n",
    "**Red: Ground Truth Values **\n",
    "\n",
    "<span style=\"color:blue\"></style>\n",
    "**Blue: Predicted Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 500\n",
    "dimension=2\n",
    "## Plotting the Outputs vs Inputs\n",
    "if (dimension==1):\n",
    "        test_input= np.random.randn(n_test,1) ## Randomly Genrated Inputs\n",
    "        gtruth = np.sin(test_input)\n",
    "else:\n",
    "## Breaking the inputs such that, the first input goes to the sin curve and the rest adds on as linear functions        \n",
    "        x= np.random.randn(n_inputs,dimension) ## Randomly Genrated Inputs\n",
    "        x1=np.reshape(x[:,0],(n_inputs,1)) \n",
    "        x2 = np.sum(x[:,1:],axis=1,keepdims= True) \n",
    "        gtruth= np.sin(x1) + x2\n",
    "# plt.plot(test_input1,gtruth,'r.')\n",
    "plt.plot(x,gtruth,'r.',markersize=12)\n",
    "\n",
    "# Caluclating Predictions from the network\n",
    "predict= feed_forward(x,tr[1],tr[2])['al']\n",
    "\n",
    "plt.plot(x,predict ,'b.')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the Error\n",
    "plt.title('Final Error  ' + str(tr[0][-1]))\n",
    "plt.plot(tr[0])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Inferences\n",
    "1. For nDimensions find the right hyperparamters becomes equally difficlut. \n",
    "2. Increasing the hidden units or hidden layers did not help get better accuracy.\n",
    "3. Although, by keeping 25 and 15 hidden units in 2 hidden layers, I was able to get a moderatley accurate result, which is better than using a single hiiden layer as in the previous question.\n",
    "4. Changing the dimension, will require to tune the hyperparamters again, but  I could not find any significant connection between dimension and paramter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B) Comments on HyperParamter Tuning\n",
    "\n",
    "* From my experience, I would say tuning the shallow network is much easier than deep netwrok.\n",
    "    * This is because of very unexpected behavior of deep netwroks with change in any hyper parameter.\n",
    "    * For example, changing the hidden unit in a layer by a single value can give an accurate output or an entirely faulty output.\n",
    "* For the linear functions we used in the above questions, I think using a shallow network is better, as it is able to predict an accurate function. Deep metwroks can be used for more complex functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Experiments to see if deep netwroks take more time to converge?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Shallow Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import exp\n",
    "\n",
    "## Defining the inputs\n",
    "## hidden_in_each_layer is such that: the number of entries\n",
    "# define the number of hidden layers and each entered value is the number of hidden units in that layer\n",
    "(n_inputs,dimension,hidden_in_each_layer) =(100,1,[1,2,2]) ## Use [3,6,5] for good accuracy with 3 layers\n",
    "np.random.seed(1)                                          ## Use [3,6,6,6,2] for good accuracy with 5 layers\n",
    "\n",
    "# Initializing the Network\n",
    "net = initialize_network(n_inputs,dimension, hidden_in_each_layer)\n",
    "\n",
    "## Hyper Parameter Initialization\n",
    "epochs= 5000\n",
    "lr = 0.0001\n",
    "# net[0]\n",
    "# print(net[1])\n",
    "\n",
    "## Train\n",
    "tr = train (net,epochs,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Graphs \n",
    "<span style=\"color:red\"></style>\n",
    "**Red: Ground Truth Values **\n",
    "\n",
    "<span style=\"color:blue\"></style>\n",
    "**Blue: Predicted Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_test = 500\n",
    "dimension=1\n",
    "## Plotting the Outputs vs Inputs\n",
    "if (dimension==1):\n",
    "        test_input= np.random.randn(n_test,1) ## Randomly Genrated Inputs\n",
    "        gtruth = np.sin(test_input)\n",
    "else:\n",
    "## Breaking the inputs such that, the first input goes to the sin curve and the rest adds on as linear functions        \n",
    "        x= np.random.randn(n_inputs,dimension) ## Randomly Genrated Inputs\n",
    "        x1=np.reshape(x[:,0],(n_inputs,1)) \n",
    "        x2 = np.sum(x[:,1:],axis=1,keepdims= True) \n",
    "        y= np.sin(x1) + x2\n",
    "plt.plot(test_input,gtruth,'r.',markersize=12)\n",
    "\n",
    "# Caluclating Predictions from the network\n",
    "predict= feed_forward(test_input,tr[1],tr[2])['al']\n",
    "\n",
    "plt.plot(test_input,predict ,'b.')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the Error\n",
    "plt.title('Final Error  ' + str(tr[0][-1]))\n",
    "plt.plot(tr[0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Deep Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from math import exp\n",
    "\n",
    "## Defining the inputs\n",
    "## hidden_in_each_layer is such that: the number of entries\n",
    "# define the number of hidden layers and each entered value is the number of hidden units in that layer\n",
    "(n_inputs,dimension,hidden_in_each_layer) =(100,1,[3,6,5,6,6,2]) ## Use [3,6,5] for good accuracy with 3 layers\n",
    "np.random.seed(1)                                          ## Use [3,6,6,6,2] for good accuracy with 5 layers\n",
    "\n",
    "# Initializing the Network\n",
    "net = initialize_network(n_inputs,dimension, hidden_in_each_layer)\n",
    "\n",
    "## Hyper Parameter Initialization\n",
    "epochs= 3500\n",
    "lr = 0.0001\n",
    "# net[0]\n",
    "# print(net[1])\n",
    "\n",
    "## Train\n",
    "tr = train (net,epochs,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Graphs \n",
    "<span style=\"color:red\"></style>\n",
    "**Red: Ground Truth Values **\n",
    "\n",
    "<span style=\"color:blue\"></style>\n",
    "**Blue: Predicted Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_test = 500\n",
    "dimension=1\n",
    "## Plotting the Outputs vs Inputs\n",
    "if (dimension==1):\n",
    "        test_input= np.random.randn(n_test,1) ## Randomly Genrated Inputs\n",
    "        gtruth = np.sin(test_input)\n",
    "else:\n",
    "## Breaking the inputs such that, the first input goes to the sin curve and the rest adds on as linear functions        \n",
    "        x= np.random.randn(n_inputs,dimension) ## Randomly Genrated Inputs\n",
    "        x1=np.reshape(x[:,0],(n_inputs,1)) \n",
    "        x2 = np.sum(x[:,1:],axis=1,keepdims= True) \n",
    "        y= np.sin(x1) + x2\n",
    "plt.plot(test_input,gtruth,'r.',markersize=12)\n",
    "\n",
    "# Caluclating Predictions from the network\n",
    "predict= feed_forward(test_input,tr[1],tr[2])['al']\n",
    "\n",
    "plt.plot(test_input,predict ,'b.')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the Error\n",
    "plt.title('Final Error  ' + str(tr[0][-1]))\n",
    "plt.plot(tr[0])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences for comapring shallow network to deep network.\n",
    "* Keeping all the hyper paramters constant other then that epochs and and hidden layer (hidden units), the deep network converged to an error of 0.17 in 3500 epochs, whereas the shallow network converge to an error of 0.18 in 5000 epochs. \n",
    "* Also, the accuracy for deep netwrok is much better than the shallow network.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
