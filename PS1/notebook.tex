
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Q4}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \subsection{Question 4: Implementation of Deep neural netwrok for
classification.}\label{question-4-implementation-of-deep-neural-netwrok-for-classification.}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Test your network with two 1D problems; that is, your input is just a
  scalar. First, choose two sets of points that are linearly separable.
  Vary the margin between the points and the number of layers in the
  network. Is it more dif- ficult to find hyperparameters that solve a
  problem with a smaller margin? Does the speed with which the network
  converges to a good solution de- pends on the margin? Include a plot
  to support your answer to the second question. Second, test your
  network with some 1D data that is not linearly separable. What
  differences do you observe?
\end{enumerate}

    \textbf{Sol:} Here we implement designing a deep neural network for
classification.\\
\#\#\#\# Loss Function Loss function as given in the question.

\paragraph{Input}\label{input}

Points that are linearly separable with a margin.

\paragraph{Outputs}\label{outputs}

\textbf{1-D data} The oputputs for 1D linearly separable data is
generated by x\textgreater{}5 for lower margin and x\textgreater{}25 for
higher margin. For linearly inseaparable data, the function y = sin(x)
is used.

\textbf{Higher Dimensional Data (here n=3)} For Linearly Seperable, I
have used equation of a plane. x1+x2+x3 +c = 0 For Linearly
Inseaparable, I have used equation of a circle. x1\^{}2 + x2\^{}2 =
r\^{}2 \#\#\#\# Hyper Paramters For this function there are four hyper
paramters, that is the \textbf{Learning Rate} (Step Size), the number of
\textbf{epochs} , the number of \textbf{hidden layers} and the number of
\textbf{hidden units} in each hidden layer.

\paragraph{The Netwrok has 6 main
Functions}\label{the-netwrok-has-6-main-functions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Network Initialization
\item
  nonlin
\item
  feed\_forward
\item
  back\_prop
\item
  update\_network
\item
  train
\end{enumerate}

\textbf{Now lets got thorugh each function}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Initialize the Network}
        \PY{k}{def} \PY{n+nf}{initialize\PYZus{}network}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{n}{dimension}\PY{p}{,} \PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{,}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{}\PYZsh{} Initializing Weights and Biases by using dictionary}
            \PY{n}{n\PYZus{}hidden\PYZus{}layers} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{)}
            \PY{n}{wts}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{dimension}\PY{p}{,}\PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{p}{\PYZcb{}}
            \PY{n}{b} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{\PYZcb{}}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}hidden\PYZus{}layers}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                \PY{n}{wts}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{]}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,}\PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}      
                \PY{n}{b}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{wts}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{n\PYZus{}hidden\PYZus{}layers}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)} 
            \PY{n}{b}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{n\PYZus{}hidden\PYZus{}layers}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{k}{return} \PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{wts}\PY{p}{,}\PY{n}{b}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{} Non Linear Function RELU}
        \PY{k}{def} \PY{n+nf}{nonlin}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{deriv} \PY{o}{=} \PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{p}{(}\PY{n}{deriv} \PY{o}{==} \PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
                \PY{k}{return} \PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{n}{x}\PY{o}{\PYZgt{}}\PY{l+m+mi}{0}\PY{p}{)}
            \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{maximum}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{x}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{} Activation function for the final layer}
        \PY{k}{def} \PY{n+nf}{transfer}\PY{p}{(}\PY{n}{activation}\PY{p}{)}\PY{p}{:}
            \PY{k}{return} \PY{l+m+mf}{1.0} \PY{o}{/} \PY{p}{(}\PY{l+m+mf}{1.0} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{activation}\PY{p}{)}\PY{p}{)}
        
        \PY{k}{def} \PY{n+nf}{transfer\PYZus{}derivative}\PY{p}{(}\PY{n}{output}\PY{p}{)}\PY{p}{:}
            \PY{k}{return} \PY{n}{output} \PY{o}{*} \PY{p}{(}\PY{l+m+mf}{1.0} \PY{o}{\PYZhy{}} \PY{n}{output}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{} Feed Forward Function}
        \PY{k}{def} \PY{n+nf}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{wts}\PY{p}{,}\PY{n}{b}\PY{p}{)}\PY{p}{:}
            \PY{n}{a} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{wts}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{n}{b}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{wts}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
                \PY{n}{a}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{nonlin}\PY{p}{(}\PY{p}{(}\PY{n}{a}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{wts}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{n}{b}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}\PY{p}{)}
            \PY{n}{a}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{al}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{transfer}\PY{p}{(}\PY{n}{a}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{wts}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{wts}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{wts}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{o}{+} \PY{n}{b}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{wts}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)} 
            \PY{k}{return} \PY{n}{a}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{} Back Propogation by appling the four main formulas of back propogation}
        \PY{k}{def} \PY{n+nf}{back\PYZus{}prop}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{a}\PY{p}{,}\PY{n}{wts}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{}     error = \PYZhy{}(((y\PYZhy{}a[\PYZsq{}al\PYZsq{}])**2).sum())                    \PYZsh{}\PYZsh{} Calculating the error}
            \PY{n}{error} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{p}{(}\PY{n}{y}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{a}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{al}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{+} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{y}\PY{p}{)}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{a}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{al}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{)} 
            \PY{n}{d} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{d}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{wts}\PY{p}{)}\PY{p}{)}\PY{p}{:} \PY{p}{(}\PY{n}{a}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{al}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{y}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{n}{transfer\PYZus{}derivative}\PY{p}{(}\PY{n}{a}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{al}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{\PYZcb{}}                \PY{c+c1}{\PYZsh{}\PYZsh{} Delta for final layer}
            \PY{n}{wt\PYZus{}err} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wr}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{wts}\PY{p}{)}\PY{p}{)}\PY{p}{:}\PY{n}{a}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{wts}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{d}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{d}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{wts}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{p}{\PYZcb{}} \PY{c+c1}{\PYZsh{}\PYZsh{} delta w for final layer}
            \PY{n}{b\PYZus{}err} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{br}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{wts}\PY{p}{)}\PY{p}{)}\PY{p}{:}\PY{n}{d}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{d}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{wts}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}                          \PY{c+c1}{\PYZsh{}\PYZsh{} delta b for final layer}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{reversed}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{wts}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{n}{d} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{d}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{wts}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{d}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{d}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{p}{(}\PY{n}{nonlin}\PY{p}{(}\PY{n}{a}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]}\PY{p}{,}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                \PY{k}{if} \PY{p}{(}\PY{n}{i}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                    \PY{n}{wt\PYZus{}err}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wr}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{d}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{d}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]}\PY{p}{)}
                    \PY{n}{b\PYZus{}err}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{br}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{d}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{d}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{keepdims}\PY{o}{=} \PY{k+kc}{True}\PY{p}{)} 
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{wt\PYZus{}err}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wr}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{a}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{d}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{d}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]}\PY{p}{)}
                    \PY{n}{b\PYZus{}err}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{br}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{d}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{d}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{]}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{keepdims}\PY{o}{=} \PY{k+kc}{True}\PY{p}{)} 
        \PY{c+c1}{\PYZsh{}     print(error)}
            \PY{k}{return} \PY{n}{wt\PYZus{}err}\PY{p}{,} \PY{n}{b\PYZus{}err}\PY{p}{,} \PY{n}{d}\PY{p}{,} \PY{n}{error}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{} Updating the Network}
        \PY{k}{def} \PY{n+nf}{update\PYZus{}network}\PY{p}{(}\PY{n}{wts}\PY{p}{,}\PY{n}{b}\PY{p}{,}\PY{n}{wt\PYZus{}err}\PY{p}{,}\PY{n}{b\PYZus{}err}\PY{p}{,}\PY{n}{lr}\PY{p}{)}\PY{p}{:}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{wts}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                \PY{n}{wts}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{lr} \PY{o}{*} \PY{n}{wt\PYZus{}err}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wr}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}
                \PY{n}{b}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{lr} \PY{o}{*} \PY{n}{b\PYZus{}err}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{br}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}
            \PY{k}{return} \PY{n}{wts}\PY{p}{,}\PY{n}{b}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{} Training the Network}
        \PY{k}{def} \PY{n+nf}{train}\PY{p}{(}\PY{n}{net}\PY{p}{,}\PY{n}{epochs}\PY{p}{,}\PY{n}{lr}\PY{p}{)}\PY{p}{:}
            \PY{n}{error} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{p}{(}\PY{n}{wts}\PY{p}{,}\PY{n}{b}\PY{p}{)} \PY{o}{=} \PY{p}{(}\PY{n}{net}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{n}{net}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
                \PY{n}{a} \PY{o}{=} \PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{net}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{wts}\PY{p}{,}\PY{n}{b}\PY{p}{)}
                \PY{n}{err} \PY{o}{=} \PY{n}{back\PYZus{}prop}\PY{p}{(}\PY{n}{net}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{net}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{a}\PY{p}{,}\PY{n}{wts}\PY{p}{)}
                \PY{n}{error}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{err}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
                \PY{n}{update} \PY{o}{=} \PY{n}{update\PYZus{}network}\PY{p}{(}\PY{n}{wts}\PY{p}{,}\PY{n}{b}\PY{p}{,}\PY{n}{err}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{err}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{lr}\PY{p}{)}
                \PY{p}{(}\PY{n}{wts}\PY{p}{,}\PY{n}{b}\PY{p}{)}\PY{o}{=} \PY{p}{(}\PY{n}{update}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{update}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
            \PY{k}{return} \PY{n}{error}\PY{p}{,} \PY{n}{update}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{update}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
\end{Verbatim}


    \subsection{(a)Defining the Main Function and Implementing for 1D input
wherein the output is linearly seperable about x= 10 and Margin is
less.}\label{adefining-the-main-function-and-implementing-for-1d-input-wherein-the-output-is-linearly-seperable-about-x-10-and-margin-is-less.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} Importing the Libraries}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         \PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k}{import} \PY{n}{pyplot} \PY{k}{as} \PY{n}{plt}
         \PY{k+kn}{from} \PY{n+nn}{math} \PY{k}{import} \PY{n}{exp}
         \PY{k+kn}{import} \PY{n+nn}{math} \PY{k}{as} \PY{n+nn}{math}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} Defining the inputs}
         \PY{c+c1}{\PYZsh{}\PYZsh{} hidden\PYZus{}in\PYZus{}each\PYZus{}layer is such that: the number of entries}
         \PY{c+c1}{\PYZsh{} define the number of hidden layers and each entered value is the number of hidden units in that layer}
         \PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{n}{dimension}\PY{p}{,}\PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{)} \PY{o}{=}\PY{p}{(}\PY{l+m+mi}{200}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)} 
         
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} Setting the Training Data}
         \PY{n}{train\PYZus{}input} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
         \PY{n}{y} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{n}{train\PYZus{}input}\PY{o}{\PYZgt{}}\PY{l+m+mi}{5}\PY{p}{)} 
         
         
         
         
         \PY{c+c1}{\PYZsh{} Initializing the Network}
         \PY{n}{net} \PY{o}{=} \PY{n}{initialize\PYZus{}network}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{n}{dimension}\PY{p}{,} \PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{,}\PY{n}{train\PYZus{}input}\PY{p}{,}\PY{n}{y}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} Hyper Parameter Initialization}
         \PY{n}{epochs}\PY{o}{=} \PY{l+m+mi}{100000}
         \PY{n}{lr} \PY{o}{=} \PY{l+m+mf}{0.001}
         \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} net[0]}
         \PY{c+c1}{\PYZsh{} print(net[1])}
         
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} Train}
         \PY{n}{tr} \PY{o}{=} \PY{n}{train} \PY{p}{(}\PY{n}{net}\PY{p}{,}\PY{n}{epochs}\PY{p}{,}\PY{n}{lr}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} net[1]}
         
         \PY{c+c1}{\PYZsh{} Plotting the Error}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Final Error  }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_4_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Plotting the Graphs}\label{plotting-the-graphs}

\textbf{Red: Ground Truth Values }

\textbf{Blue: Predicted Values}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{n\PYZus{}inputs} \PY{o}{=} \PY{l+m+mi}{200}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} Plotting the Outputs vs Inputs}
         \PY{n}{test\PYZus{}input} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
         \PY{n}{gtruth} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{o}{\PYZgt{}}\PY{l+m+mi}{5}\PY{p}{)} 
             
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{,}\PY{n}{gtruth}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Caluclating Predictions from the network}
         \PY{n}{predicted} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{,}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{al}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{acc}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{n}{gtruth} \PY{o}{==} \PY{n}{predicted}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy is : }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{acc}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{,}\PY{n}{predicted}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         
         \PY{c+c1}{\PYZsh{} gtruth}
         \PY{c+c1}{\PYZsh{} feed\PYZus{}forward(test\PYZus{}input,tr[1],tr[2])[\PYZsq{}al\PYZsq{}]}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_6_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Implementing for 1D input and wherein the output is linearly
seperable about x= 25 and Margin is
high.}\label{implementing-for-1d-input-and-wherein-the-output-is-linearly-seperable-about-x-25-and-margin-is-high.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{c+c1}{\PYZsh{} Importing the Libraries}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         \PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k}{import} \PY{n}{pyplot} \PY{k}{as} \PY{n}{plt}
         \PY{k+kn}{from} \PY{n+nn}{math} \PY{k}{import} \PY{n}{exp}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} Defining the inputs}
         \PY{c+c1}{\PYZsh{}\PYZsh{} hidden\PYZus{}in\PYZus{}each\PYZus{}layer is such that: the number of entries}
         \PY{c+c1}{\PYZsh{} define the number of hidden layers and each entered value is the number of hidden units in that layer}
         \PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{n}{dimension}\PY{p}{,}\PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{)} \PY{o}{=}\PY{p}{(}\PY{l+m+mi}{200}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)} 
         
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} Setting the Training Data}
         \PY{n}{train\PYZus{}input} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
         \PY{n}{y} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{n}{train\PYZus{}input}\PY{o}{\PYZgt{}}\PY{l+m+mi}{25}\PY{p}{)} 
         
         \PY{c+c1}{\PYZsh{} Initializing the Network}
         \PY{n}{net} \PY{o}{=} \PY{n}{initialize\PYZus{}network}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{n}{dimension}\PY{p}{,} \PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{,}\PY{n}{train\PYZus{}input}\PY{p}{,}\PY{n}{y}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} Hyper Parameter Initialization}
         \PY{n}{epochs}\PY{o}{=} \PY{l+m+mi}{100000}
         \PY{n}{lr} \PY{o}{=} \PY{l+m+mf}{0.0001}
         \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} net[0]}
         \PY{c+c1}{\PYZsh{} print(net[1])}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} Train}
         \PY{n}{tr} \PY{o}{=} \PY{n}{train} \PY{p}{(}\PY{n}{net}\PY{p}{,}\PY{n}{epochs}\PY{p}{,}\PY{n}{lr}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Plotting the Error}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Final Error  }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_8_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Plotting the Graphs}\label{plotting-the-graphs}

\textbf{Red: Ground Truth Values }

\textbf{Blue: Predicted Values}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}67}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Plotting the Outputs vs Inputs}
         \PY{n}{test\PYZus{}input} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
         \PY{n}{gtruth} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{o}{\PYZgt{}}\PY{l+m+mi}{25}\PY{p}{)} 
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{,}\PY{n}{gtruth}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Caluclating Predictions from the network}
         \PY{n}{predicted} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{,}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{al}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{acc}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{n}{gtruth} \PY{o}{==} \PY{n}{predicted}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy is : }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{acc}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{,}\PY{n}{predicted}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         
         \PY{c+c1}{\PYZsh{} gtruth}
         \PY{c+c1}{\PYZsh{} feed\PYZus{}forward(test\PYZus{}input,tr[1],tr[2])[\PYZsq{}al\PYZsq{}]}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_10_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{(a) Important Comaprisons when data is being separated
between large and small
margin.}\label{a-important-comaprisons-when-data-is-being-separated-between-large-and-small-margin.}

\paragraph{For Small margin: (all x\textgreater{}5 = 1 \&\& all
x\textless{} 5 = 0)}\label{for-small-margin-all-x5-1-all-x-5-0}

epochs= 100,000, lr = 0.001; final error = 0.72 with 100 per accuracy.
\#\#\#\# For Large margin: (all x\textgreater{}25 = 1 \&\& all
x\textless{} 25 = 0) epochs= 100,000, lr = 0.0001, final error = 9.2
with 96.5 per accuracy.

\textbf{From the above results, we can see that the higher the margin,
the less is the accuracy and more is the error at the end of
convergence.}

\subsection{Therefore a lower margin will take less time to converge
than a higher margin
input.}\label{therefore-a-lower-margin-will-take-less-time-to-converge-than-a-higher-margin-input.}

(For this experiment, the hidden layers tuning is very important, even
after repeated trials, I was only able to make the network converge with
2 hidden layers.

    \section{Testing with data that is not linearly
separable.}\label{testing-with-data-that-is-not-linearly-separable.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{c+c1}{\PYZsh{} Importing the Libraries}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         \PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k}{import} \PY{n}{pyplot} \PY{k}{as} \PY{n}{plt}
         \PY{k+kn}{from} \PY{n+nn}{math} \PY{k}{import} \PY{n}{exp}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} Defining the inputs}
         \PY{c+c1}{\PYZsh{}\PYZsh{} hidden\PYZus{}in\PYZus{}each\PYZus{}layer is such that: the number of entries}
         \PY{c+c1}{\PYZsh{} define the number of hidden layers and each entered value is the number of hidden units in that layer}
         \PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{n}{dimension}\PY{p}{,}\PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{)} \PY{o}{=}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} 
         
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} Setting the Training Data}
         \PY{c+c1}{\PYZsh{} Taking the function y= sin(x)}
         \PY{n}{train\PYZus{}input} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
         \PY{n}{y} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{train\PYZus{}input}\PY{p}{)}\PY{o}{\PYZgt{}}\PY{l+m+mi}{0}\PY{p}{)} 
         
         \PY{c+c1}{\PYZsh{} Initializing the Network}
         \PY{n}{net} \PY{o}{=} \PY{n}{initialize\PYZus{}network}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{n}{dimension}\PY{p}{,} \PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{,}\PY{n}{train\PYZus{}input}\PY{p}{,}\PY{n}{y}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} Hyper Parameter Initialization}
         \PY{n}{epochs}\PY{o}{=} \PY{l+m+mi}{100000}
         \PY{n}{lr} \PY{o}{=} \PY{l+m+mf}{0.0001}
         \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} net[0]}
         \PY{c+c1}{\PYZsh{} print(net[1])}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} Train}
         \PY{n}{tr} \PY{o}{=} \PY{n}{train} \PY{p}{(}\PY{n}{net}\PY{p}{,}\PY{n}{epochs}\PY{p}{,}\PY{n}{lr}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Plotting the Error}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Final Error  }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_13_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}89}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Plotting the Outputs vs Inputs}
         \PY{n}{test\PYZus{}input} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
         \PY{n}{gtruth} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{)}\PY{o}{\PYZgt{}}\PY{l+m+mi}{0}\PY{p}{)} 
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{,}\PY{n}{gtruth}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Caluclating Predictions from the network}
         \PY{n}{predicted} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{,}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{al}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{acc}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{n}{gtruth} \PY{o}{==} \PY{n}{predicted}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy is : }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{acc}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{,}\PY{n}{predicted}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         
         \PY{c+c1}{\PYZsh{} gtruth}
         \PY{c+c1}{\PYZsh{} feed\PYZus{}forward(test\PYZus{}input,tr[1],tr[2])[\PYZsq{}al\PYZsq{}]}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_14_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    For linearly inseparable data, I was unable to tune the network so that
the error converge to zero. I was able to get a maximum convergence to
error = 69.2 in 100,000 epochs and 0.0001 learning rate.

Also, we can see that the accuracy is as low as, 62 percent. Therefore,
either we tune the network such that it converges better or we increase
the number of dimensions and try.

** In all, it is much easier to converge linearly separable data than
inseparabe data.**

    \section{(b) For Higher Dimension of
Input}\label{b-for-higher-dimension-of-input}

\subsection{Linearly Separable Data}\label{linearly-separable-data}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}109}]:} \PY{c+c1}{\PYZsh{} Importing the Libraries}
          \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
          \PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k}{import} \PY{n}{pyplot} \PY{k}{as} \PY{n}{plt}
          \PY{k+kn}{from} \PY{n+nn}{math} \PY{k}{import} \PY{n}{exp}
          
          \PY{c+c1}{\PYZsh{}\PYZsh{} Defining the inputs}
          \PY{c+c1}{\PYZsh{}\PYZsh{} hidden\PYZus{}in\PYZus{}each\PYZus{}layer is such that: the number of entries}
          \PY{c+c1}{\PYZsh{} define the number of hidden layers and each entered value is the number of hidden units in that layer}
          \PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{n}{dimension}\PY{p}{,}\PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{)} \PY{o}{=}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} 
          
          
          \PY{c+c1}{\PYZsh{}\PYZsh{} Setting the Training Data}
          \PY{c+c1}{\PYZsh{} Taking the function y= x1+x2+x3+c}
          \PY{n}{train\PYZus{}input1} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
          \PY{n}{train\PYZus{}input2} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
          \PY{n}{train\PYZus{}input3} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
          \PY{n}{train\PYZus{}input} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(}\PY{n}{train\PYZus{}input1}\PY{p}{,}\PY{n}{train\PYZus{}input2}\PY{p}{,}\PY{n}{train\PYZus{}input3}\PY{p}{)}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{c+c1}{\PYZsh{} print(train\PYZus{}input.shape)}
          \PY{n}{y} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{p}{(}\PY{n}{train\PYZus{}input1}\PY{o}{+}\PY{n}{train\PYZus{}input2}\PY{o}{+}\PY{n}{train\PYZus{}input3}\PY{p}{)}\PY{o}{\PYZgt{}}\PY{l+m+mi}{0}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Initializing the Network}
          \PY{n}{net} \PY{o}{=} \PY{n}{initialize\PYZus{}network}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{n}{dimension}\PY{p}{,} \PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{,}\PY{n}{train\PYZus{}input}\PY{p}{,}\PY{n}{y}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{}\PYZsh{} Hyper Parameter Initialization}
          \PY{n}{epochs}\PY{o}{=} \PY{l+m+mi}{70000}
          \PY{n}{lr} \PY{o}{=} \PY{l+m+mf}{0.001}
          \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{c+c1}{\PYZsh{} net[0]}
          \PY{c+c1}{\PYZsh{} print(net[1])}
          
          \PY{c+c1}{\PYZsh{}\PYZsh{} Train}
          \PY{n}{tr} \PY{o}{=} \PY{n}{train} \PY{p}{(}\PY{n}{net}\PY{p}{,}\PY{n}{epochs}\PY{p}{,}\PY{n}{lr}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Plotting the Error}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Final Error  }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_17_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}106}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Plotting the Outputs vs Inputs}
          \PY{c+c1}{\PYZsh{} Taking the function y= x1+x2+x3+c}
          \PY{n}{test\PYZus{}input1} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
          \PY{n}{test\PYZus{}input2} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
          \PY{n}{test\PYZus{}input3} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
          \PY{n}{test\PYZus{}input} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(}\PY{n}{train\PYZus{}input1}\PY{p}{,}\PY{n}{train\PYZus{}input2}\PY{p}{,}\PY{n}{train\PYZus{}input3}\PY{p}{)}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{c+c1}{\PYZsh{} print(train\PYZus{}input.shape)}
          \PY{n}{y} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{p}{(}\PY{n}{test\PYZus{}input1}\PY{o}{+}\PY{n}{test\PYZus{}input2}\PY{o}{+}\PY{n}{test\PYZus{}input3}\PY{p}{)}\PY{o}{\PYZgt{}}\PY{l+m+mi}{0}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Caluclating Predictions from the network}
          \PY{n}{predicted} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{,}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{al}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
          \PY{n}{acc}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{n}{y} \PY{o}{==} \PY{n}{predicted}\PY{p}{)}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy is : }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{acc}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{,}\PY{n}{predicted}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
          
          
          \PY{c+c1}{\PYZsh{} gtruth}
          \PY{c+c1}{\PYZsh{} feed\PYZus{}forward(test\PYZus{}input,tr[1],tr[2])[\PYZsq{}al\PYZsq{}]}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_18_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    For Higher Dimensions, and linearly separable data, I was able to
converge the network in 70,000 epochs using 3 hidden layers. The error
is 0.8. Finding the parmaters were relatively easier for high dimension
linearly separable data.

    \subsection{Linearly Inseparable}\label{linearly-inseparable}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}165}]:} \PY{c+c1}{\PYZsh{} Importing the Libraries}
          \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
          \PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k}{import} \PY{n}{pyplot} \PY{k}{as} \PY{n}{plt}
          \PY{k+kn}{from} \PY{n+nn}{math} \PY{k}{import} \PY{n}{exp}
          
          \PY{c+c1}{\PYZsh{}\PYZsh{} Defining the inputs}
          \PY{c+c1}{\PYZsh{}\PYZsh{} hidden\PYZus{}in\PYZus{}each\PYZus{}layer is such that: the number of entries}
          \PY{c+c1}{\PYZsh{} define the number of hidden layers and each entered value is the number of hidden units in that layer}
          \PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{n}{dimension}\PY{p}{,}\PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{)} \PY{o}{=}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)} 
          
          
          \PY{c+c1}{\PYZsh{}\PYZsh{} Setting the Training Data}
          \PY{c+c1}{\PYZsh{} Taking the function y= x1\PYZca{}2 + x\PYZca{}2 = r\PYZca{}2 (circle)}
          \PY{n}{train\PYZus{}input1} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
          \PY{n}{train\PYZus{}input2} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
          \PY{n}{train\PYZus{}input} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(}\PY{n}{train\PYZus{}input1}\PY{p}{,}\PY{n}{train\PYZus{}input2}\PY{p}{)}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{c+c1}{\PYZsh{} print(train\PYZus{}input.shape)}
          \PY{n}{y} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{p}{(}\PY{n}{train\PYZus{}input1}\PY{o}{*}\PY{n}{train\PYZus{}input1}\PY{o}{+}\PY{n}{train\PYZus{}input2}\PY{o}{*}\PY{n}{train\PYZus{}input2}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1500}\PY{o}{\PYZgt{}}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} predicting points outside the circle)}
          \PY{c+c1}{\PYZsh{} Initializing the Network}
          \PY{n}{net} \PY{o}{=} \PY{n}{initialize\PYZus{}network}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{n}{dimension}\PY{p}{,} \PY{n}{hidden\PYZus{}in\PYZus{}each\PYZus{}layer}\PY{p}{,}\PY{n}{train\PYZus{}input}\PY{p}{,}\PY{n}{y}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{}\PYZsh{} Hyper Parameter Initialization}
          \PY{n}{epochs}\PY{o}{=} \PY{l+m+mi}{60000}
          \PY{n}{lr} \PY{o}{=} \PY{l+m+mf}{0.001}
          \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{c+c1}{\PYZsh{} net[0]}
          \PY{c+c1}{\PYZsh{} print(net[1])}
          
          \PY{c+c1}{\PYZsh{}\PYZsh{} Train}
          \PY{n}{tr} \PY{o}{=} \PY{n}{train} \PY{p}{(}\PY{n}{net}\PY{p}{,}\PY{n}{epochs}\PY{p}{,}\PY{n}{lr}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Plotting the Error}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Final Error  }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} train\PYZus{}input1}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_21_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}160}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Plotting the Outputs vs Inputs}
          \PY{c+c1}{\PYZsh{} Taking the function y= x1\PYZca{}2 + x\PYZca{}2 = r\PYZca{}2 (circle)}
          \PY{n}{test\PYZus{}input1} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
          \PY{n}{test\PYZus{}input2} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{n}{n\PYZus{}inputs}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{n\PYZus{}inputs}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{}\PYZsh{} Randomly Genrated Inputs}
          \PY{n}{test\PYZus{}input} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(}\PY{n}{train\PYZus{}input1}\PY{p}{,}\PY{n}{train\PYZus{}input2}\PY{p}{)}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{c+c1}{\PYZsh{} print(train\PYZus{}input.shape)}
          \PY{n}{y} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{p}{(}\PY{n}{train\PYZus{}input1}\PY{o}{*}\PY{n}{train\PYZus{}input1}\PY{o}{+}\PY{n}{train\PYZus{}input2}\PY{o}{*}\PY{n}{train\PYZus{}input2}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1500}\PY{o}{\PYZgt{}}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} predicting points outside the circle)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Caluclating Predictions from the network}
          \PY{n}{predicted} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{feed\PYZus{}forward}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{,}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{tr}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{al}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
          \PY{n}{acc}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{*}\PY{p}{(}\PY{n}{y} \PY{o}{==} \PY{n}{predicted}\PY{p}{)}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy is : }\PY{l+s+s1}{\PYZsq{}}\PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{acc}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{test\PYZus{}input}\PY{p}{,}\PY{n}{predicted}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
          
          
          \PY{c+c1}{\PYZsh{} gtruth}
          \PY{c+c1}{\PYZsh{} feed\PYZus{}forward(test\PYZus{}input,tr[1],tr[2])[\PYZsq{}al\PYZsq{}]}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    For higher dimension, tuning is hyper paramters for linearly inseparable
data is very difficult. After parameter tuning the network converged to
an error of 8.95 with 60,000 epchs and 4 hidden layers. The accuracy is
56 percent. The error convergence is better than 1D linearly inseaprable
data.

\begin{itemize}
\tightlist
\item
  \textbf{In conclusion, setting hyper paramters for linearly
  inseparable data is much difficlut and also the convergence time and
  accuracy is less.}
\end{itemize}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
