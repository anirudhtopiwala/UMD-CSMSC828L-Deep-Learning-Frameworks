{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify the data shown in the MNIST data\n",
    "\n",
    "\n",
    "* Do this using the best practices discussed in class (i.e. one hot encoding, ReLU, CNNS when you should, etc.)\n",
    "\n",
    "* Have reasonable hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from keras.layers import Dense, Dropout, Lambda, Flatten, Activation, Dropout\n",
    "# from mnist import MNIST #python-mnist package (available only through pip)\n",
    "# import os\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Training and Testing Data\n",
    "trainImages = np.load('./MNIST/trainImages.npy')\n",
    "testImages = np.load('./MNIST/testImages.npy')\n",
    "trainLabels = np.load('./MNIST/trainLabels.npy')\n",
    "testLabels = np.load('./MNIST/testLabels.npy')\n",
    "\n",
    "# Converting to Float32 to do input data Normalization\n",
    "trainImages = trainImages.astype('float32') #images loaded in as int64, 0 to 255 integers\n",
    "testImages = testImages.astype('float32')\n",
    "# Normalization\n",
    "trainImages /= 255\n",
    "testImages /= 255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing the Input Data after Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview the training data\n",
    "plt.figure(figsize=(12,10))\n",
    "x, y = 3, 3\n",
    "for i in range(6):  \n",
    "    plt.subplot(y, x, i+1)\n",
    "    plt.imshow(trainImages[i].reshape((28,28)),interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Hyper Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "batch_size = 128\n",
    "num_classes = testLabels.shape[1]\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 26, 26)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 62, 24, 32)        7520      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 31, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 11904)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1523840   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,533,290\n",
      "Trainable params: 1,533,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "img_rows, img_cols = 28, 28\n",
    "input_shape = (1,img_rows, img_cols)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                 activation='relu',data_format='channels_first',use_bias=True ,\n",
    "#                  bias_initializer = keras.initializers.glorot_uniform(seed=None),\n",
    "                 input_shape=input_shape))          # Adding weights for tuning\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))    # Added another layer as this \n",
    "                                                    # smoothens the training accuracy curve\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))           # Reducing the parmaters\n",
    "model.add(Dropout(0.25))                            # Adding non linearity and randomness\n",
    "model.add(Flatten())                                # Converting to 1D as required before  \n",
    "                                                    # fully connected layer.\n",
    "model.add(Dense(128, activation='relu'))            # fully connected layer with 128 hidden units\n",
    "model.add(Dropout(0.45))                            # Adding non linearity and randomness\n",
    "model.add(Dense(num_classes, activation='softmax')) # Final fully connected layer.\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "# Get Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and Biases for First Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights for final layer has dimensions of (3, 3, 1, 64) and the values are :\n",
      "[[[[ 7.73506612e-03 -3.36870104e-02 -9.20380577e-02 -8.31639022e-03\n",
      "     6.99063540e-02  6.89742714e-02  7.48668313e-02  8.11859667e-02\n",
      "     2.22502351e-02 -5.25011048e-02 -5.40901497e-02  9.57949907e-02\n",
      "    -6.35766834e-02  2.62200981e-02 -6.53174669e-02  6.03310913e-02\n",
      "     1.01186574e-01  4.88142669e-02  4.92558181e-02  4.22023982e-03\n",
      "    -1.79251060e-02 -3.08542699e-02  3.58691216e-02 -6.04547597e-02\n",
      "    -9.47608352e-02  2.97503322e-02 -9.50900614e-02  2.09717453e-02\n",
      "    -4.30191830e-02 -9.18820798e-02 -5.55499680e-02  5.12414724e-02\n",
      "    -2.31821761e-02 -3.68061811e-02  3.97760868e-02  5.66947609e-02\n",
      "    -6.02897480e-02 -9.97055396e-02 -4.70537879e-02  1.00991532e-01\n",
      "     3.01654190e-02 -7.38711357e-02  8.60292614e-02 -2.48726457e-03\n",
      "    -7.81009793e-02 -1.27048045e-02  8.48108232e-02  7.48784095e-02\n",
      "     9.23638046e-02 -8.50086957e-02  9.93431360e-02 -3.48410457e-02\n",
      "    -4.97685261e-02  8.84522647e-02  7.56131411e-02 -7.48328045e-02\n",
      "    -7.30646476e-02 -5.75693138e-02  9.19429660e-02  4.19249684e-02\n",
      "     4.35360372e-02 -9.79641825e-03 -1.21714547e-02  2.91544199e-02]]\n",
      "\n",
      "  [[ 1.47214457e-02  8.23966712e-02  6.81102872e-02  1.53755248e-02\n",
      "     8.46840143e-02 -6.67698234e-02 -2.90521979e-03 -6.34101555e-02\n",
      "     5.11263758e-02 -3.66764739e-02  4.88091260e-02  8.72976333e-03\n",
      "     6.90305680e-02  9.51322615e-02 -4.68802042e-02 -2.55557150e-03\n",
      "    -3.10583264e-02  5.97429425e-02 -8.42315704e-02 -9.12231207e-02\n",
      "    -7.67560005e-02 -3.27165723e-02  4.40464318e-02 -9.41459686e-02\n",
      "     2.06929371e-02 -6.67441785e-02  1.85158253e-02  5.76017350e-02\n",
      "    -5.65181337e-02 -1.78178772e-02 -6.18735775e-02  2.73692012e-02\n",
      "    -3.45546529e-02  9.30765569e-02  4.15805876e-02  8.40066373e-02\n",
      "    -2.07813084e-02  3.69238257e-02  4.73899245e-02  2.33821794e-02\n",
      "    -7.59208500e-02  8.16101581e-03  4.14977968e-02  2.86291540e-03\n",
      "     7.47008771e-02  7.21962750e-02  5.67515194e-03  6.55432940e-02\n",
      "    -6.46892488e-02  1.57922804e-02 -2.48285681e-02  3.42953801e-02\n",
      "    -8.02683830e-03  6.29748851e-02 -8.95747766e-02  7.84210116e-02\n",
      "    -1.17942765e-02 -7.59429485e-02 -1.16996989e-02 -2.96671987e-02\n",
      "    -5.14125489e-02  5.85957617e-03 -5.24877757e-02  1.52129009e-02]]\n",
      "\n",
      "  [[-2.15115473e-02 -3.25493142e-02  2.69646645e-02 -8.35122764e-02\n",
      "    -8.00964981e-02  6.00837320e-02  7.69667178e-02 -4.57408950e-02\n",
      "     9.42021608e-02  1.00723341e-01  7.08742589e-02  5.41481674e-02\n",
      "     7.31089711e-03 -4.77716364e-02 -4.87546995e-02  2.95330435e-02\n",
      "    -8.49053264e-04  1.01214379e-01  7.32780397e-02  6.41716123e-02\n",
      "     9.95688140e-04  1.19420737e-02 -2.40209252e-02 -9.87247229e-02\n",
      "     9.42460001e-02 -8.73687342e-02  5.08431941e-02  8.00556093e-02\n",
      "     8.39097649e-02  9.52649415e-02  6.79975748e-02 -2.50155777e-02\n",
      "     8.35762620e-02  3.80645096e-02 -2.82655284e-02 -2.19494253e-02\n",
      "    -5.47873080e-02 -9.04880837e-02 -3.94947715e-02 -7.30039179e-02\n",
      "    -3.54595110e-02  3.07072252e-02  6.96499497e-02  1.10950395e-02\n",
      "    -7.96936005e-02 -7.00840428e-02  3.78583968e-02  3.18326950e-02\n",
      "    -6.56752288e-02 -5.66963032e-02 -9.59214196e-02  8.50996077e-02\n",
      "     5.04550785e-02  8.11759531e-02 -8.51596594e-02 -2.93506086e-02\n",
      "    -5.64032011e-02  7.41248280e-02 -3.98262404e-02 -3.09453458e-02\n",
      "    -9.66649577e-02 -3.98211218e-02 -5.49100377e-02  9.71999764e-03]]]\n",
      "\n",
      "\n",
      " [[[-5.29870354e-02 -2.20203176e-02 -8.02159682e-02  7.50542134e-02\n",
      "     6.71947747e-03 -6.87982887e-02  6.71623796e-02  6.88437074e-02\n",
      "    -8.75815526e-02 -7.50793293e-02  8.84515941e-02  2.99786329e-02\n",
      "    -3.75698805e-02  5.85651547e-02  1.29963607e-02  2.54108608e-02\n",
      "     7.31827915e-02 -3.85913402e-02  7.67545998e-02 -2.59712338e-03\n",
      "    -1.64478794e-02  2.55064517e-02  7.67923892e-02 -4.30081934e-02\n",
      "     1.16498619e-02  8.37639719e-03 -8.27093571e-02 -1.47737712e-02\n",
      "    -1.32544786e-02 -6.31262064e-02 -3.82090434e-02  3.81115377e-02\n",
      "     1.49104297e-02  1.88253745e-02 -6.53424263e-02  1.84191689e-02\n",
      "     5.96773177e-02 -6.90801442e-02  7.12101460e-02 -4.75152098e-02\n",
      "     4.47940230e-02  5.80088794e-02 -9.89634469e-02  5.81873208e-02\n",
      "     9.14494544e-02 -8.69575888e-02  5.24304509e-02 -4.19478863e-03\n",
      "    -2.79456675e-02 -2.28677094e-02 -5.91040552e-02  1.87146664e-02\n",
      "    -5.80554120e-02  8.55975300e-02 -8.74611139e-02  3.10174227e-02\n",
      "    -8.55630040e-02  7.73633868e-02 -5.98080419e-02 -8.94991010e-02\n",
      "     8.69456381e-02  5.84764183e-02  3.46773565e-02 -1.94619000e-02]]\n",
      "\n",
      "  [[ 4.15031314e-02 -8.19322467e-03 -7.37883598e-02  6.65722638e-02\n",
      "     2.45578438e-02  1.36173368e-02 -6.63466677e-02  1.07779354e-02\n",
      "    -8.55444670e-02  1.01276264e-02 -6.94144517e-03 -9.72365588e-02\n",
      "    -9.32108089e-02 -1.55552402e-02  2.36004815e-02 -1.66272819e-02\n",
      "     4.32856083e-02  9.00393277e-02  2.11990774e-02 -3.90650257e-02\n",
      "    -2.70064995e-02 -1.00068256e-01 -7.68171102e-02  2.33568251e-02\n",
      "     2.59852856e-02  8.89004767e-02 -2.59719118e-02  3.87791693e-02\n",
      "    -9.14048851e-02  5.89976162e-02 -5.46611473e-02  4.02159095e-02\n",
      "    -8.66966918e-02 -5.22907488e-02 -6.86301440e-02 -6.49362057e-03\n",
      "     2.64385939e-02  5.72435409e-02  4.84516770e-02 -9.47251692e-02\n",
      "    -2.78395712e-02  2.86017954e-02  5.94089180e-02 -7.83193111e-02\n",
      "    -7.59274960e-02 -2.73720399e-02  3.71320099e-02  2.35639662e-02\n",
      "     9.11414623e-02 -3.34462970e-02  6.76508993e-03  8.08638483e-02\n",
      "    -9.88900214e-02  3.77961546e-02 -4.56296578e-02  6.49556965e-02\n",
      "     7.93933719e-02  3.88915241e-02 -6.16288856e-02  9.30542797e-02\n",
      "    -8.75920802e-02  6.86919838e-02 -4.75317277e-02 -5.85965402e-02]]\n",
      "\n",
      "  [[ 9.26523507e-02 -5.64810000e-02  1.81476772e-02  3.67546827e-03\n",
      "     9.15976167e-02 -5.23330271e-02 -2.00204849e-02 -6.50374740e-02\n",
      "     4.93036211e-03 -3.88019532e-03 -7.01915622e-02 -1.55165121e-02\n",
      "     9.50027555e-02 -8.24754387e-02  1.29557028e-02  4.73857373e-02\n",
      "    -4.85712886e-02  9.21576321e-02 -9.85764191e-02  3.03334296e-02\n",
      "    -1.71165913e-03  7.64636695e-02 -1.80456787e-03 -8.67571980e-02\n",
      "    -7.77063742e-02 -5.09886965e-02  5.04917353e-02  4.26565111e-02\n",
      "     3.58516872e-02  2.66006589e-02 -2.36611590e-02 -4.92852256e-02\n",
      "    -8.99709761e-04 -3.23662162e-03 -5.91594204e-02  6.36674315e-02\n",
      "     7.83259720e-02  4.08276320e-02  7.79233128e-02 -3.56051102e-02\n",
      "    -8.77816752e-02  5.41683286e-02  3.99465114e-02  8.76362175e-02\n",
      "    -7.21385926e-02 -6.93246722e-04 -9.26750451e-02 -8.79652053e-02\n",
      "    -3.17705944e-02 -4.08516303e-02  1.01157293e-01 -9.08359140e-03\n",
      "     2.60329545e-02  1.82649568e-02  8.40101391e-02  8.15298706e-02\n",
      "    -1.68777928e-02 -1.96219161e-02 -2.72036791e-02  7.52388686e-02\n",
      "    -5.78604117e-02 -5.54390922e-02  3.00272405e-02 -8.55202973e-03]]]\n",
      "\n",
      "\n",
      " [[[-6.26792759e-03  2.10846290e-02 -4.10120562e-02 -7.07851127e-02\n",
      "    -3.52576748e-02  3.45794111e-02 -8.27072114e-02  5.40187806e-02\n",
      "    -5.44521175e-02  4.35930490e-02 -9.23972726e-02 -3.80550623e-02\n",
      "     1.29090250e-02 -1.07541829e-02 -4.05190513e-02  7.51657188e-02\n",
      "     3.98541987e-03 -6.39685690e-02  1.22797191e-02 -7.15071633e-02\n",
      "    -5.26130684e-02  2.89510489e-02  4.83082980e-02 -4.77867573e-03\n",
      "     5.11312336e-02 -7.11334795e-02  9.70451832e-02 -7.93176591e-02\n",
      "     3.57468873e-02  4.25470322e-02 -4.25166860e-02  7.94362873e-02\n",
      "     6.86198026e-02 -3.10268626e-02 -9.85684767e-02  5.30570000e-03\n",
      "     9.38190818e-02 -5.79521172e-02  3.89491022e-02 -7.81926364e-02\n",
      "     8.64201039e-02 -6.28694892e-02 -4.17070612e-02 -8.90022591e-02\n",
      "    -3.17066312e-02  9.50799733e-02 -8.75188410e-03 -7.83577561e-03\n",
      "     4.75880057e-02  3.04071605e-02 -4.69893925e-02 -5.88805154e-02\n",
      "    -7.94092119e-02  7.69080222e-03 -8.36178660e-05  3.01447958e-02\n",
      "     7.34204501e-02  8.29827338e-02 -7.71079957e-03  2.93715596e-02\n",
      "     5.30294180e-02  7.74125755e-02 -4.21090871e-02  1.86650679e-02]]\n",
      "\n",
      "  [[-5.60417660e-02  8.24745744e-02  7.30832368e-02  2.06833035e-02\n",
      "    -2.54845098e-02  7.05907345e-02  6.80329949e-02  1.77176967e-02\n",
      "     8.55498910e-02 -5.96891753e-02  6.78373724e-02  3.83962840e-02\n",
      "     9.71523374e-02 -6.60121068e-02 -7.36646652e-02 -7.21685588e-02\n",
      "     1.49675161e-02  3.34309489e-02  4.78800535e-02 -3.08053046e-02\n",
      "     8.77798200e-02  8.29129815e-02 -6.78634942e-02 -7.94620961e-02\n",
      "    -1.98787525e-02  1.83099136e-02 -8.47113207e-02  6.87586516e-02\n",
      "     2.21060365e-02  7.41629303e-02  9.14787948e-02  3.26317549e-02\n",
      "     2.74680406e-02 -4.32212315e-02  9.65485275e-02  9.22817737e-03\n",
      "     9.16761905e-03  1.28986686e-02  2.38060504e-02 -4.24876884e-02\n",
      "     2.45957077e-02 -7.34219551e-02 -8.83965194e-03 -6.22762740e-03\n",
      "    -6.45540953e-02 -7.07127005e-02 -2.12953687e-02 -2.94515565e-02\n",
      "    -2.76387557e-02  1.78461522e-02  2.83883363e-02 -8.62313062e-02\n",
      "     4.11069244e-02 -7.63198733e-03  3.73302400e-02 -1.93645954e-02\n",
      "     3.75603288e-02 -9.67318416e-02 -4.08483706e-02 -2.57402807e-02\n",
      "     9.99170840e-02  3.62820029e-02 -5.72581254e-02  1.65692121e-02]]\n",
      "\n",
      "  [[ 2.50463933e-02 -1.01285949e-02 -1.15162358e-02 -1.05892867e-02\n",
      "    -3.66017446e-02 -9.89501700e-02  6.38206750e-02  7.60067403e-02\n",
      "    -1.83667764e-02  1.71446204e-02  8.89696777e-02  1.81287229e-02\n",
      "     1.27489194e-02 -6.98888302e-02 -5.99533506e-02  6.06247187e-02\n",
      "    -7.96079561e-02 -1.24654770e-02 -9.55192745e-02  6.39024526e-02\n",
      "    -2.58843824e-02 -2.39612833e-02  7.18840957e-02  7.94056803e-03\n",
      "    -3.54055464e-02  6.98653311e-02  5.96503317e-02  4.81618345e-02\n",
      "     9.47166979e-02 -6.95274621e-02 -5.52537255e-02  7.06658512e-02\n",
      "    -6.69002533e-02  8.90049636e-02  3.23069245e-02 -6.97871447e-02\n",
      "    -5.67486994e-02  2.09647939e-02 -7.52741620e-02  5.01781106e-02\n",
      "     3.20932567e-02 -4.98783626e-02  2.25421786e-02  8.42107087e-02\n",
      "    -8.95171389e-02  9.26481634e-02  2.88767517e-02  9.07643586e-02\n",
      "    -3.92515287e-02 -5.27420528e-02  5.69350868e-02 -1.17611960e-02\n",
      "    -1.53386816e-02  1.36319697e-02  8.88944715e-02  9.32343751e-02\n",
      "    -7.00818747e-02  3.54664624e-02 -2.27691680e-02 -3.01753953e-02\n",
      "     2.23180354e-02  2.07475796e-02 -6.02731854e-02 -3.73550355e-02]]]]\n",
      "\n",
      "The Biases for final layer has dimensions of (64,) and the values are :\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Just so we know that the weights have changed after training, we will be comparing the sum of weights of he final layer before and after training.\n",
      "\n",
      "The sum of weights of the final layer is :0.69874007\n",
      "The sum of biases of the final layer is :0.0\n"
     ]
    }
   ],
   "source": [
    "# Before Training\n",
    "weights = model.layers[0].get_weights()\n",
    "w0 = np.array(weights[0])\n",
    "b0 = np.array(weights[1])\n",
    "print(\"The weights for final layer has dimensions of \" + str(w0.shape)+\" and the values are :\\n\" + str(w0))\n",
    "print()\n",
    "print(\"The Biases for final layer has dimensions of \" + str(b0.shape)+\" and the values are :\\n\" + str(b0))\n",
    "print()\n",
    "print(\"Just so we know that the weights have changed after training,\\\n",
    " we will be comparing the sum of weights of he final layer before and after training.\")\n",
    "print()\n",
    "print(\"The sum of weights of the final layer is :\" + str(w0.sum()))\n",
    "print(\"The sum of biases of the final layer is :\" + str(b0.sum()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor of shape [128] and type float\n\t [[Node: training/Adam/zeros_13 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [128] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'training/Adam/zeros_13', defined at:\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-ebae3e2090b5>\", line 7, in <module>\n    validation_data=(testImages, testLabels))\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\", line 1008, in fit\n    self._make_train_function()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\", line 498, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/keras/optimizers.py\", line 483, in get_updates\n    vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/keras/optimizers.py\", line 483, in <listcomp>\n    vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 700, in zeros\n    v = tf.zeros(shape=shape, dtype=tf_dtype, name=name)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1539, in zeros\n    output = _constant_if_small(zero, shape, dtype, name)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1497, in _constant_if_small\n    return constant(value, shape=shape, dtype=dtype, name=name)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 202, in constant\n    name=name).outputs[0]\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [128] and type float\n\t [[Node: training/Adam/zeros_13 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [128] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [128] and type float\n\t [[Node: training/Adam/zeros_13 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [128] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ebae3e2090b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m validation_data=(testImages, testLabels))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Evaluate Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2658\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2659\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [128] and type float\n\t [[Node: training/Adam/zeros_13 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [128] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'training/Adam/zeros_13', defined at:\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-ebae3e2090b5>\", line 7, in <module>\n    validation_data=(testImages, testLabels))\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\", line 1008, in fit\n    self._make_train_function()\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\", line 498, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/keras/optimizers.py\", line 483, in get_updates\n    vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/keras/optimizers.py\", line 483, in <listcomp>\n    vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 700, in zeros\n    v = tf.zeros(shape=shape, dtype=tf_dtype, name=name)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1539, in zeros\n    output = _constant_if_small(zero, shape, dtype, name)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1497, in _constant_if_small\n    return constant(value, shape=shape, dtype=dtype, name=name)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 202, in constant\n    name=name).outputs[0]\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/home/anirudh/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [128] and type float\n\t [[Node: training/Adam/zeros_13 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [128] values: 0 0 0...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# Train \n",
    "# Here I have considered the test data as the validation data.\n",
    "h = model.fit(trainImages, trainLabels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "validation_data=(testImages, testLabels))\n",
    "\n",
    "# Evaluate Accuracy\n",
    "\n",
    "score = model.evaluate(testImages, testLabels, verbose=1)\n",
    "print('Validation loss:', score[0])\n",
    "print(' accuracy:', score[1])\n",
    "\n",
    "# Plot Graphs\n",
    "print(h.history.keys())\n",
    "accuracy = h.history['acc']\n",
    "val_accuracy = h.history['val_acc']\n",
    "loss = h.history['loss']\n",
    "val_loss = h.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('validation accuracy :' + str(score[1]))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('validation loss:'+str(score[0]))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferences\n",
    "* There are 2 Convolution Layers and 2 Dense or Fully Connected Layers with subsequent Max Pooling and Dropout Layer.\n",
    "* The number of hidden units in each layer are experimentally fine tuned. The units were experimented in 2^n multiples like 16, 32 , 64 ,128 , 256. \n",
    "* ** Input Conv2D layer: ** The weights are initialized by Glorot uniform initializer, which is the default. This is set as the values are nicely zero centered and within -1 and 1.The biases are set to zero as it improves the accuracy by 0.2 as compared to randomly distribued values between (-1 and (found out by experimnetation).This is also recommended as per in CS231 from Standord. \n",
    "    * The kernel size is set to minimum of (3,3). This is because the image size is already small (28,28) and by using large kernel size there wont be enough data left to add more convolution layers.\n",
    "    * the activation is set to RELU, as it is the most recommended and also as the input is normalized between 0 and 1.\n",
    "* ** Max Pooling layer: ** for pooling Max Pooling is used. Again as the input size is very less. The maxpooling kernel size is set to (2,2). This works as there is still enough information in a (2,2) window and it also does not throw out a lot of valuable data.\n",
    "* ** Drop Out: ** This is very Essential as it helps prevent over fitting. Two dropout layers are used. \n",
    "    * The first one is in between convolution layers. The dropout of 0.25 was determined to be optimal. At this vlaue the convergence of Training accuracy to validation accuracy was smooth and gradual as compared to other values.\n",
    "    * The second dropout was used just before the final fully connected layer. This is kept at 0.45 as experimnetally determined and is used to reduce overfitting.\n",
    "* ** Dense Layers: ** Two fully connect layers are used with a dropput in between. This seems to give good performances in MNIST and Fashion MNIST.\n",
    "    * For final fully connected layer, softmax is used as activation. This is because there are a total of 10 classes and the predicted output can exactly be any one of these classes.\n",
    "* ** Loss ** : as one hot encoding is used, categorical_crossentropy loss is used as loss function. This is used as it is the most recommended one.\n",
    "* ** Optimizer ** : Different Optimizers were tried out. For MNIST even with different optimizers there was not a lot of change in the convergence time or the number of epochs. \n",
    "    * For example, I used Adagrad and each epoch took 3 seconds less to finish as compared to Adam, but the final validation accuracy was higher for Adam. Therefore I finally decided to use Adam as my optimizers. The default paramteres were not further tuned as the accuracy was already above 99 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Weights and Biases for First Layer After Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights for final layer has dimensions of (3, 3, 1, 64) and the values are :\n",
      "[[[[-4.80886102e-02 -1.70912668e-02 -3.56890470e-01  1.04048818e-01\n",
      "     1.98908389e-01 -2.44477034e-01 -1.48991019e-01  1.71195120e-01\n",
      "    -5.54862656e-02 -1.98491469e-01 -6.29025176e-02 -1.79533437e-01\n",
      "    -9.24052149e-02 -2.54496932e-01  1.22660853e-01 -2.90366132e-02\n",
      "     1.78159446e-01  1.02201022e-01  2.80651301e-01  2.18030751e-01\n",
      "    -3.41139853e-01 -1.32931247e-01 -3.36015463e-01  1.92337811e-01\n",
      "    -3.16966288e-02  2.44183815e-03  2.01386884e-01  1.59459457e-01\n",
      "     1.19238421e-01  6.35402277e-02  4.66045039e-03 -1.82998739e-02\n",
      "     1.76967740e-01  1.71942636e-01 -2.65411943e-01 -2.43521333e-01\n",
      "     7.99171329e-02  7.64406519e-03  1.70422509e-01 -1.78423941e-01\n",
      "    -1.36100784e-01 -2.85944104e-01 -1.66973203e-01 -5.19456446e-01\n",
      "     3.66451740e-01  1.28035560e-01  1.45382360e-01  2.75537558e-02\n",
      "    -3.68666947e-01  1.10733636e-01  1.15689300e-01 -5.09027131e-02\n",
      "    -1.07146040e-01 -9.62132514e-02  1.40851200e-01 -2.23781437e-01\n",
      "    -2.16148064e-01 -3.70151103e-01  2.06433341e-01  8.30698982e-02\n",
      "    -3.70059103e-01 -1.12883702e-01  2.35165387e-01 -2.05502942e-01]]\n",
      "\n",
      "  [[ 1.81474954e-01  2.19583865e-02 -1.72432829e-02  1.14609823e-01\n",
      "     2.15257630e-02 -2.80444305e-02  1.87950075e-01  1.70246363e-01\n",
      "    -1.58818200e-01 -2.16698155e-01  3.00544268e-03 -2.14773882e-02\n",
      "    -9.83515531e-02  8.51300359e-02  7.85150006e-02  1.34141535e-01\n",
      "     1.15945160e-01  6.68109506e-02  1.96393430e-01  1.70852125e-01\n",
      "    -1.84434116e-01 -2.13696272e-03 -5.84143221e-01  3.38598490e-01\n",
      "    -4.93379802e-01  2.42011935e-01 -1.38249725e-01  3.42158794e-01\n",
      "     1.56150013e-01 -2.15530440e-01 -5.86406514e-02 -4.74018455e-02\n",
      "     2.30862990e-01  1.09672077e-01 -1.40070319e-01 -4.12276722e-02\n",
      "     1.88966393e-01  6.54783249e-02  2.98954904e-01 -1.92667663e-01\n",
      "    -1.15097113e-01 -1.13883698e-02 -2.05137178e-01 -2.93175071e-01\n",
      "     3.22922856e-01  1.91536158e-01  2.36503795e-01  1.59085959e-01\n",
      "    -1.73680112e-01 -6.76666275e-02  9.50016081e-02 -8.67911652e-02\n",
      "    -3.57834041e-01 -7.61051700e-02  6.48390129e-02 -1.36560351e-02\n",
      "    -6.30915388e-02 -1.50712445e-01 -1.20195802e-02 -7.51885697e-02\n",
      "    -4.26820546e-01 -2.09663033e-01  2.91654676e-01 -1.94146276e-01]]\n",
      "\n",
      "  [[ 1.37427986e-01  1.53634235e-01  1.23985365e-01  1.36025354e-01\n",
      "    -7.85078034e-02  4.69127260e-02  1.73232660e-01 -1.70549363e-01\n",
      "     2.55347788e-01  9.09522250e-02  8.39742497e-02 -2.75840424e-02\n",
      "    -8.74558091e-03  1.76624984e-01  2.24310711e-01 -3.75597291e-02\n",
      "    -7.88389593e-02 -7.02437162e-02 -2.79444516e-01  2.11393777e-02\n",
      "     1.59069180e-01 -8.69135708e-02 -2.36064225e-01  3.72799873e-01\n",
      "    -5.75167835e-01  3.37781310e-01 -2.40645796e-01 -1.82926767e-02\n",
      "     1.94921806e-01 -5.35699800e-02 -1.99753493e-01  6.12856038e-02\n",
      "     2.05215320e-01  1.55609315e-02 -5.95175475e-02  2.67827511e-01\n",
      "    -6.04519760e-03  1.71266664e-02  4.06971186e-01 -9.74595547e-02\n",
      "    -1.20347135e-01  2.95031637e-01 -1.75222978e-01  1.67619050e-01\n",
      "     2.23781228e-01  3.06731611e-01  3.63684356e-01  3.85227539e-02\n",
      "     1.43523039e-02  2.30374243e-02  7.21609667e-02 -1.26869511e-03\n",
      "    -1.45613346e-02  1.68052807e-01  1.67405568e-02 -1.32726073e-01\n",
      "    -1.77435413e-01 -1.03515252e-01 -1.56745329e-01 -2.53083199e-01\n",
      "    -3.81107986e-01 -9.95075107e-02  2.05243811e-01 -1.71800137e-01]]]\n",
      "\n",
      "\n",
      " [[[-1.24569148e-01  5.14646731e-02  1.23900667e-01  1.33272156e-01\n",
      "    -9.89698246e-02  9.68403146e-02  1.79695591e-01 -5.27082868e-02\n",
      "    -1.96760491e-01  8.45348649e-03  1.37820050e-01  1.04796819e-01\n",
      "    -2.70086303e-02 -1.24408394e-01 -6.53826864e-03  1.82678208e-01\n",
      "    -4.01777029e-02 -3.93413845e-03  2.05373392e-01  8.96160379e-02\n",
      "    -3.04921597e-01  1.63548574e-01  4.57971357e-03 -5.42937517e-02\n",
      "     3.85424495e-01 -2.82919675e-01  2.78748751e-01 -1.02161810e-01\n",
      "     4.81689498e-02  4.05619852e-03  2.36584112e-01  2.35479817e-01\n",
      "     1.71081811e-01  4.71035875e-02 -7.67435804e-02 -2.02225059e-01\n",
      "     4.86633480e-02  1.51291907e-01  1.72540650e-01  2.30195180e-01\n",
      "    -1.88320756e-01 -5.64920962e-01 -2.61747450e-01 -2.07236707e-01\n",
      "     2.13185385e-01  1.39269352e-01  1.37692079e-01 -2.03156099e-02\n",
      "    -2.93282628e-01  1.29834414e-01  3.30872275e-02 -4.60620299e-02\n",
      "    -1.84973821e-01 -2.51500728e-03  1.81299821e-01  2.90831476e-01\n",
      "     2.30098471e-01 -9.63744596e-02  1.43738151e-01  4.96522896e-02\n",
      "     1.46055520e-01 -9.58168730e-02  2.13895719e-02 -1.44340783e-01]]\n",
      "\n",
      "  [[-9.10128057e-02 -3.05968020e-02  2.31524393e-01 -1.49717303e-02\n",
      "    -2.83489883e-01  2.09914207e-01  2.47802854e-01  1.39271304e-01\n",
      "    -2.69787699e-01 -8.18508267e-02  1.43299773e-01  1.12302355e-01\n",
      "    -8.11322927e-02  4.68242764e-02  1.39226809e-01  2.44174555e-01\n",
      "     6.48707477e-03 -3.34047861e-02  1.55192241e-01  1.38195932e-01\n",
      "    -5.07433787e-02  1.16920687e-01 -1.33458272e-01  1.76266015e-01\n",
      "     5.84165789e-02  3.01232236e-03 -1.70812413e-01  5.41616008e-02\n",
      "     1.27775416e-01  2.95980182e-02  2.83028912e-02  8.89874399e-02\n",
      "     1.02003545e-01  1.09496631e-01 -5.85178472e-02  9.37757865e-02\n",
      "     7.56370202e-02 -6.02366440e-02  7.16971010e-02  2.46661827e-02\n",
      "     5.13438582e-02 -2.77775414e-02 -1.74535528e-01  2.80302972e-01\n",
      "    -7.13395402e-02  1.02940619e-01  5.70935756e-02  1.72831878e-01\n",
      "    -1.55413654e-04  3.69170643e-02  2.02146798e-01 -1.62507519e-02\n",
      "     2.78368453e-03  4.37525064e-02  2.89952904e-01  2.81685829e-01\n",
      "     8.69816318e-02  3.94842550e-02  2.87902802e-02 -7.95950592e-02\n",
      "     7.06598610e-02 -2.29996517e-01  4.25757617e-02  9.80749577e-02]]\n",
      "\n",
      "  [[ 4.76346351e-02  1.25330776e-01  2.95986772e-01  1.29206389e-01\n",
      "    -2.73679435e-01  2.14063585e-01  5.22996373e-02 -1.65946767e-01\n",
      "     1.24259405e-01  1.18458621e-01  1.46438032e-01  6.55376315e-02\n",
      "    -8.94031674e-02  5.37087694e-02  1.21103816e-01  1.49584219e-01\n",
      "    -6.34367913e-02 -1.13982432e-01 -1.64342478e-01  3.60205695e-02\n",
      "     3.18129867e-01 -2.33251881e-02 -1.82289243e-01  2.91228145e-01\n",
      "    -1.72709748e-01  2.47025833e-01 -2.42973730e-01  2.24856019e-01\n",
      "     1.06675802e-02  3.19097727e-03 -6.40903264e-02  2.21629485e-01\n",
      "     9.94415022e-03  1.25050351e-01  2.71203041e-01  1.82428986e-01\n",
      "     1.61186129e-01 -1.88886642e-01  1.38921499e-01  7.34150857e-02\n",
      "     8.28546509e-02  3.51636678e-01  9.23139229e-02  2.06348985e-01\n",
      "    -1.56152695e-01 -5.04999198e-02 -8.08951259e-02  4.82986532e-02\n",
      "     3.44276458e-01  2.08698120e-02  1.95879161e-01 -3.31456289e-02\n",
      "     1.65805623e-01  1.91561580e-01  2.55105525e-01  5.90786226e-02\n",
      "     7.01227188e-02  4.93495315e-02 -1.35913923e-01 -7.90186110e-04\n",
      "     2.40086854e-01 -2.81564772e-01  9.53907594e-02  2.56850153e-01]]]\n",
      "\n",
      "\n",
      " [[[-8.29243511e-02 -1.30994454e-01  1.72830731e-01 -1.37155607e-01\n",
      "    -3.39920700e-01 -5.18139377e-02  1.29292697e-01 -1.04274005e-01\n",
      "    -1.40934736e-01 -3.66659850e-01  3.48766372e-02  9.67267603e-02\n",
      "    -6.18886203e-02 -1.09546416e-01 -1.19479887e-01 -6.19910844e-02\n",
      "    -1.08361438e-01 -2.03332633e-01  4.95059900e-02 -2.01290503e-01\n",
      "    -1.91030562e-01 -1.18987650e-01  4.35343623e-01 -5.99218369e-01\n",
      "     3.53511989e-01 -3.29060793e-01  2.55524725e-01 -4.17133301e-01\n",
      "    -2.52195746e-01 -9.01309177e-02  9.93096158e-02  3.40137295e-02\n",
      "    -2.61065543e-01  9.78414342e-03 -8.64557847e-02 -1.44842699e-01\n",
      "     6.13729320e-02 -2.23649219e-01 -2.94805408e-01  2.62789130e-01\n",
      "     1.96963772e-02 -2.81905651e-01 -7.42718652e-02  2.05872387e-01\n",
      "    -2.18111813e-01 -1.99938789e-01 -2.17183828e-01 -1.07437670e-01\n",
      "    -3.10080290e-01  8.42197314e-02 -1.93988651e-01  7.90756419e-02\n",
      "    -2.23026812e-01 -2.36238450e-01 -8.45559761e-02  9.84681100e-02\n",
      "     9.89088789e-02 -1.24979950e-03  2.12080613e-01 -2.56042600e-01\n",
      "     2.38563672e-01  2.15488538e-01 -2.12944031e-01  7.34481663e-02]]\n",
      "\n",
      "  [[-1.42280370e-01 -1.46020189e-01  1.22094870e-01 -1.18301854e-01\n",
      "    -5.88197298e-02  1.33888036e-01  9.49562266e-02  1.03308581e-01\n",
      "    -1.48601085e-01 -2.35705256e-01  7.14366585e-02  3.49862836e-02\n",
      "    -6.11928664e-02 -3.06516569e-02 -1.14495277e-01  1.07408591e-01\n",
      "    -1.88101679e-01 -2.70623714e-01  1.08995751e-01 -1.16691045e-01\n",
      "     2.22949177e-01 -1.56467944e-01  4.50188965e-01 -6.27649784e-01\n",
      "     2.60143757e-01 -4.07665581e-01  8.88243690e-02  6.77843541e-02\n",
      "    -2.85131097e-01 -1.44522607e-01  8.58125761e-02 -5.28194308e-02\n",
      "    -2.10061103e-01  3.62256318e-02  3.72070707e-02 -9.24527645e-02\n",
      "     1.14341564e-01 -1.93947077e-01 -5.12222409e-01  1.58118516e-01\n",
      "     2.25377753e-01 -6.11762777e-02  1.63897559e-01  2.87976295e-01\n",
      "    -5.88655114e-01 -3.75551492e-01 -3.47146153e-01  1.28460303e-01\n",
      "    -1.64161861e-01  6.76747188e-02  8.81620310e-03  1.78500935e-01\n",
      "    -6.19557250e-06 -1.22552700e-01 -8.31085294e-02  1.59128904e-01\n",
      "     1.82944700e-01  9.73557606e-02  3.09884489e-01 -3.47837731e-02\n",
      "     2.21235052e-01  3.00166309e-01 -1.87792003e-01  1.52980059e-01]]\n",
      "\n",
      "  [[-2.06011564e-01 -1.34192631e-01 -2.05686688e-02 -9.57308337e-02\n",
      "    -2.56496996e-01  8.19045529e-02 -2.34203249e-01 -1.13977037e-01\n",
      "    -2.47510220e-03  3.16900872e-02 -4.47373614e-02  2.35371571e-02\n",
      "    -6.64796159e-02  5.69300540e-02 -2.75897048e-02 -1.12722203e-01\n",
      "     2.25886088e-02 -1.69565916e-01 -7.93525577e-02  2.45895004e-03\n",
      "     3.03477407e-01 -1.01883672e-01  1.81386635e-01 -1.46888196e-01\n",
      "     2.48695493e-01 -1.37759700e-01 -4.31162380e-02  2.48487040e-01\n",
      "    -1.26073211e-01 -1.83980420e-01  1.27290308e-01 -2.35587731e-02\n",
      "    -5.97159155e-02  4.13042828e-02  9.89006385e-02  8.61728713e-02\n",
      "     2.38183320e-01  5.45187593e-02 -1.81502342e-01  9.29548498e-03\n",
      "     1.78181186e-01  2.78321832e-01  2.02232957e-01  2.74581704e-02\n",
      "    -2.60165632e-01 -5.04533112e-01 -5.30626774e-01 -1.62459418e-01\n",
      "     1.62589118e-01  3.25655229e-02  1.91557333e-01 -8.60087648e-02\n",
      "     1.12077065e-01  3.39370966e-02  2.06931122e-02  1.10390685e-01\n",
      "     6.08645706e-03  1.95236906e-01  1.19131304e-01  1.95058152e-01\n",
      "     3.14785898e-01  2.49744073e-01 -2.46516794e-01  3.11920404e-01]]]]\n",
      "\n",
      "The Biases for final layer has dimensions of (64,) and the values are :\n",
      "[ 0.04731046  0.04039197 -0.01332885 -0.03656422  0.06549946  0.02577718\n",
      " -0.0091886   0.03370367 -0.01927429  0.04007134 -0.01748665 -0.01308215\n",
      "  0.         -0.06442433  0.01340184 -0.03010272  0.01171735  0.05291838\n",
      " -0.03967526 -0.01200262 -0.03038413  0.07233141  0.01250622  0.02022916\n",
      " -0.04303008  0.01478529 -0.025877   -0.02736124  0.02099083  0.03369354\n",
      " -0.00304239 -0.00564966  0.02497552 -0.03362538  0.02432596  0.04639511\n",
      " -0.06322051  0.06111475  0.00698193  0.01221235  0.04050666 -0.03217755\n",
      " -0.02149276 -0.03765127 -0.01083206  0.03836229  0.02975511 -0.02455418\n",
      "  0.02389825 -0.04123263 -0.01726374  0.033379   -0.02871925  0.04745006\n",
      " -0.03438209 -0.00568526  0.03355727  0.03961376 -0.00261968  0.06931432\n",
      "  0.01062256  0.01758756 -0.012919    0.02881084]\n",
      "\n",
      "The sum of weights of the final layer is :1.7024043\n",
      "The sum of biases of the final layer is :0.33734187\n",
      "As we can see the sum has changed, indicating that the weights are now tuned.\n"
     ]
    }
   ],
   "source": [
    "# After Training\n",
    "weights = model.layers[0].get_weights()\n",
    "w0 = np.array(weights[0])\n",
    "b0 = np.array(weights[1])\n",
    "print(\"The weights for final layer has dimensions of \" + str(w0.shape)+\" and the values are :\\n\" + str(w0))\n",
    "print()\n",
    "print(\"The Biases for final layer has dimensions of \" + str(b0.shape)+\" and the values are :\\n\" + str(b0))\n",
    "print()\n",
    "print(\"The sum of weights of the final layer is :\" + str(w0.sum()))\n",
    "print(\"The sum of biases of the final layer is :\" + str(b0.sum()))\n",
    "print(\"As we can see the sum has changed, indicating that the weights are now tuned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix is :\n",
      "[[ 977    1    0    1    0    0    1    0    0    0]\n",
      " [   0 1132    1    1    0    0    1    0    0    0]\n",
      " [   1    1 1024    1    1    0    0    3    1    0]\n",
      " [   0    0    1 1006    0    1    0    1    0    1]\n",
      " [   0    0    2    0  975    0    2    0    0    3]\n",
      " [   1    0    0   12    0  876    1    1    0    1]\n",
      " [   4    3    1    0    2    2  945    0    1    0]\n",
      " [   0    2    5    0    0    0    0 1020    1    0]\n",
      " [   2    1    1    1    0    0    0    3  964    2]\n",
      " [   0    1    0    6    8    0    0    4    1  989]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(testImages)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred, axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(testLabels, axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "print(\"Confusion Matrix is :\\n\"+str(confusion_mtx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorrect Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Incorrect Predicitons are: 9908 out of 10000 inputs\n",
      "\n",
      "Displaying the top 6 errors\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Errors are difference between predicted labels and true labels\n",
    "errors = (Y_pred_classes - Y_true != 0)\n",
    "incorrect = errors.size - (errors*1).sum()\n",
    "print (\"Number of Incorrect Predicitons are: \" + str(incorrect)+ \" out of \"+ str (errors.size)+\" inputs\")\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_val_errors = testImages[errors]\n",
    "\n",
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    fig.tight_layout()\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],\n",
    "                                                                               obs_errors[error]))\n",
    "            n += 1\n",
    "            \n",
    "    \n",
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "print()\n",
    "print(\"Displaying the top 6 errors\")\n",
    "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
